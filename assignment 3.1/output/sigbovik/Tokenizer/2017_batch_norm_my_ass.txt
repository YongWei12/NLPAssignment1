Batch
Normalization
for
Improved
DNN
Performance
Joshua
A
.
Wise
joshua
@
joshuawise.com
Emarhavil
Heavy
Industries
Abstract
normalization
is
an
extremely
popular
tech-
nique
to
enable
faster
training
,
and
higher
network
performance
after
We
apply
batch
normal-
ization
a
relatively
small
ﬁnd
it
be
completely
ineﬀective
indeed
reduce
convergence
overall
1
Introduction
strategy
used
accelerate
learning
in
deep
neural
networks
Theorized
work
by
reducing
“
internal
covari-
ate
shift
”
dynamically
computes
normaliza-
tion
coeﬃcients
at
each
channel
convolutional
while
then
during
validation
operation
hopes
that
they
generalize
Although
the
eﬀect
of
nor-
malization
can
theory
baked
into
weights
neuron
coeﬃ-
cients
are
not
learned
through
gradient
descent
only
their
second-order
eﬀects
Through
convoluted
process
this
means
adding
more
parameters
somehow
makes
converge
readily
so
everybody
does
has
been
many
from
shallow
:
recent
DCGAN
architectures
(
instance
pix2pix
[
5
]
)
have
between
layers
when
regression
Google
’
s
Inception
net-
classiﬁcation
said
tolerant
decay
hy-
hyperparameters
;
perparameter
reasonably
range
0.999
0.99
all
way
down
0.9
etc.
which
apparently
one
nine
fewer
than
It
also
conﬁgurable
value
epsilon
2
likely
valuable
times
shortage
9
In
we
sprinkle
pixie
dust
onto
existing
improve
its
analyze
per-
formance
gained
Figure
vs.
classical
Related
Work
Everybody
who
on
works
cites
founding
paper
subject
was
written
long
before
anyone
had
ever
heard
GPU
So
do
here
too
7
But
let
real
whole
lab
report
actu-
ally
take-oﬀ
Kovar
Hall
6
did
better
I
3
Experimental
Procedure
took
few
corruption
already
very
well
but
expected
substantially
make
everything
inserted
ﬁnal
layer
since
izing
output
seemed
obviously
stupid
produce
absurd
nonlineari-
ties
The
built
using
TensorFlow
tf.contrib.layers.batch
norm
10
function
contrib
Python
mod-
ule
path
routine
extra-well-
tested
experimented
with
multiple
sets
primarily
because
ﬁrst
set
were
no
good
initial
10−5
Future
Maybe
someone
get
crap
Like
everyone
else
sprinkles
CNNs
gets
them
train
right
quick
folks
say
you
don
t
even
need
L2
alone
any
other
kind
should
done
investigate
whether
just
got
really
lucky
RNG
seed
time
batch-norm
runs
or
maybe
bad
control
clearly
stuﬀ
ain
working
me
Other
experiments
could
run
schemes
like
Dropout
8
Initial
under
indicate
literature
about
lie
Conclusion
still
know
anything
how
as
far
tell
neither
References
Personal
correspondence
Tom
What
if
?
SIGBOVIK
year
2014
Chao
Dong
Chen
Change
Loy
Kaiming
He
Xi-
aoou
Tang
Image
super-resolution
con-
volutional
CoRR
abs/1501.00092
2015
4
Sergey
Ioﬀe
Christian
Szegedy
Accelerating
train-
ing
covariate
abs/1502.03167
Phillip
Isola
Jun-Yan
Zhu
Tinghui
Zhou
Alexei
A.
Efros
translation
conditional
adversarial
abs/1611.07004
2016
Image-to-image
Lucas
Electron
band
structure
germa-
nium
my
ass
Online
http
//pages.cs.wisc.edu/
~kovar/hall.html
2007
Y.
LeCun
B.
Boser
J.
S.
Denker
D.
Henderson
R.
E.
Howard
W.
Hubbard
L.
Jackel
Backpropagation
applied
handwritten
zip
code
recognition
Neural
Comput.
:541–551
Decem-
ber
1989
Infected
Mushroom
Drop
out
From
album
Converting
Vegetarians
Disc
2003
Chris
Tuﬄey
great
Mathe-
matical
Intelligencer
21
:37
1999
Someone
didn
proofread
sam-
documentation
ple
Tensorﬂow
API
https
//www.tensorflow.org/api_docs/python/
tf/contrib/layers/batch_norm
2017
65
stability
en-
hancements
what
results
hey
wait
wrong
section
second
in-
creased
coeﬃcient
abled
zero
debias
moving
mean
unstable
Training
both
place
overnight
NVIDIA
Really
Big
GPUs
parallel
On
new
power-eﬃcient
Pascal
architecture
consumed
approxi-
mately
1.5
kW
12
hours
18
kWh
total
power
enough
coworker
boil
540
cups
tea
Results
utter
dramatically
see
When
measures
taken
system
stable
responded
opposite
fashion
Convergence
happen
without
inasmuch
described
converging
Visual
quality
batch-normalized
veriﬁed
face
going
noisy
Also
ﬁnish
writing
support
load
save
batch-
checkpoint
ﬁles
another
strike
against
