Degenerative
Adversarial
Networks
Raphael
Gontijo
Lopes∗
&
Diptodip
Deb
Georgia
Tech
,
Atlanta
GA
{
raphaelgontijolopes
diptodipdeb
}
@
gatech.edu
Abstract
In
recent
years
Deep
Learning
researchers
have
collectively
achieved
a
pace
of
useful
information
extraction
that
is
dangerously
close
to
outstripping
the
second
law
thermodynamics
.
To
solve
this
problem
we
propose
new
framework
for
estimating
degenerative
models
via
an
adversarial
process
in
which
simulta-
neously
train
two
:
network
D
destroys
data
distri-
bution
and
discriminative
model
estimates
probability
sample
came
from
true
noise
rather
than
D.
The
training
procedure
maximize
making
mistake
Within
space
arbitrary
roll
D20
check
damage
This
system
corresponds
entropy
maximiza-
tion
ensures
timely
heat
death
Experiments
would
demonstrated
potential
but
most
our
results
were
degenerated
running
them
Introduction
promise3
deep
learning
discover
rich
hierarchical
[
5
]
represent
distributions
over
differ-
ent
kinds
such
as
natural
images
audio
waveforms
con-
taining
speech
symbols
language
corpora
(
see
Figure
1
)
All
structuring
works
decrease
en-
tropy
by
creating
discriminators
are
able
classify
into
well-deﬁned
labels
Furthermore
now
suc-
cess
generative
due
Goodfellow
et
al
Kingma
8
further
accelerates
structured
generation
power
photos
Bobolas
2009
Maley
2011
10
discovery
too
much
At
rate
will
outstrip
begin
universe
2
order
maintain
reality
know
it
present
Network
or
DAN
sidesteps
successes
steady
healthy
towards
sweet
release
proposed
degeneration
pitted
against
adversary
attempts
distinguish
whether
actual
has
been
de-
generated
observed
can
be
thought
team
steamrollers
ﬂattening
uniform
distribution
maximum
protractors
trying
determine
if
its
input
properly
ﬂattened
Competition
game
drive
both
groups
improve
until
discriminator
not
reliably
between
Currently
looking
grad
school
Please
accept
me
your
lab
Plot
showing
dangers
hindsight
plot
generates
so
please
refrain
at
give
speciﬁc
algorithms
discrimination
We
explore
case
passing
through
multilayer
after
being
perturbed
also
perceptron
refer
special
show
networks
using
backpropagation
end-
to-end
fashion
Uniquely
approach
there
no
need
actually
code
Related
Work
Training
infamously
hard
12
be-
cause
optimization
objective
equates
ﬁnd
Nash
equilibrium
non-cooperative
found
even
more
complicated
when
degenerating
makes
very
easy
Degenera-
tor
output
PhD
students4
some
work
ﬁeld
argued
balance
adversaries
needs
stabilize
avoid
local
minima
How-
ever
4
shows
how
robust
solution
involves
overpowered
Discrimina-
values
collaboration
past
one
another
’
s
differences
come
play
Therefore
proceed
with
technique
Our
methods
described
below
3
Example
bad
Equilib-
rium
degenerate
result
candidate
”
on
his
thesis
6
use
techniques
However
their
method
limited
only
trick
other
neural
While
slowing
down
creation
sufﬁcient
objectives
preserve
enough
structure
humans
still
discriminate
ease
human
Degeneration
b
DANs
Comparison
presented
ours
Note
well
whereas
4with
apologies
degenerates
Nets
3.1
Description
following
role-playing
domD
subD
V
=
EGeof
f
log
eep
+
EHinton
−
addy
Some
might
say
notation
confusing
Those
people
wrong
■
practice
Equation
provides
absolutely
all
either
think
OK
reproducibility
least
important
aspect
science
straightforward
apply
straight
copy
paste
someone
else
As
learn
degenerator
pd
x
don
t
do
anything
just
except
ignore
given
replace
Python
random
module
GAN
top
architectures
bottom
former
used
generate
latter
See
approximately
probably
equally
formal
explanation
include
here
basically
reader
next
section
theoretical
about
essen-
tially
criterion
above
allows
lose
nets
trained
simultaneously
updating
blue
dashed
line
discriminates
samples
original
black
dotted
px
those
green
solid
lower
horizontal
domain
higher
part
destroy
upward
arrows
mapping
z
imposes
transformed
later
overwrite
learns
scatter
uniformly
Consider
pair
before
divergence
similar
pdata
accurate
inner
loop
algorithm
ruined
starts
diverge
started
disperse
heavily
unlearning
c
After
update
gradient
longer
exists
reached
uniformity
d
several
steps
capacity
map
anymore
learned
own
hence
self
what
call
“
super-uniformity
boost
high
counteract
others
A
key
step
requires
mention
ﬁgure
never
rest
paper
3.2
Method
download
out-of-the-box
7
repurpose
–
mean
dataset
without
modifying
single
Algorithm
Input
Internet
connection
Output
Noise
Open
browser
Go
www.google.com
Type
google.
Click
Google
gan
tensorﬂow
git
clone
Generate
noisy
Train
9
Leave
counter
15
minutes
cool
Overwrite
believe
generalized
involve
optimiz-
ing
search
strategy
e.g
you
type
infogan
leave
discussion
future
Additionally
overwriting
manually-written
disk
writes
end-to-end
tackling
task
AI
ethics
alarmists
claim
accelerate
impeding
doom
civilization
13
reasonable
alternative
thus
posit
worthwhile
pursuit
subﬁelds
Machine
like
MNIST
ImageNet
For
purposes
randomness
ones
cryptography
desperate
bid
citations
existing
options
favor
fabricating
DANOISE6
discuss
implications
decision
When
keep
mind
must
Unfortunately
availability
scarce
deterministic
Turing
machine
settle
python
approximation
It
crucial
note
however
does
validity
based
fact
seek
If
readily
available
simple
import
then
point
;
Theoretical
Results
implicitly
deﬁnes
nothing
converge
equivalent
diverging
maximizes
organized
manner
meaning
4.1
global
optimum
puniform
Global
Optimality
Entropy
ﬁrst
consider
optimal
any
Proposition
ﬁxed
gets
overwritten
D∗
≡
/dev/urandom
Proof
whichever
really
matter
What
ﬁnal
simply
ceases
exist
accomplished
since
Theorem
minimum
occurs
reaches
max-
imum
reaching
enabling
write
optima
saddle
way
elegant
proof
itchy
forcing
us
abstain
including
plan
proofs
margins
small
4.2
Divergence
Regardless
each
meaningless
always
o
h
‘
w
wou
l
i
k
e
lost
re
accepting
suggestions
acronyms
justify
name
Sample
24
epochs
Random
taken
examples
look
visually
source
non-approximation
seen
correct
ve
established
state
art
deﬁnitive
destroyed
preliminary
experiments
novel
metric
Data
percentage
%
dd
compare
Table
Model
VGG
Inception
Britney
Spears
MP3s
AlphaGo
DANOISE
dataset7
set
my
family
album
different
permanent
clear
comparison
architecture
inherently
superior
these
because
bold
font
highlighting
Conclusion
entirely
new8
achieves
With
get
closer
stopping
Big
terror
maintaining
hope
inspired
help
conclude
presenting
few
potentially
interesting
research
directions
Inspired
InfoGAN
InfoDAN
property
uninterpretability
latent
structural
features
composed
Similarly
Radford
11
DCDAN
same
twice
number
convolution
layers
half
many
learnable
parameters
Lastly
Chen
EntropyDAN
could
representation
8i.e
plagiarized
repurposed
References
brain-neurons
Xi
Yan
Duan
Rein
Houthooft
John
Schulman
Ilya
Sutskever
Pieter
Abbeel
In-
fogan
Interpretable
maximizing
Advances
Neural
Information
Processing
Systems
pages
2172–2180
2016
Jia
Deng
Wei
Dong
Richard
Socher
Li-Jia
Li
Kai
Fei-Fei
Imagenet
large-scale
image
database
Computer
Vision
Pattern
Recognition
CVPR
IEEE
Conference
248–255
Ian
Nips
tutorial
Generative
arXiv
preprint
arXiv:1701.00160
Jean
Pouget-Abadie
Mehdi
Mirza
Bing
Xu
David
Warde-Farley
Sherjil
Ozair
Aaron
Courville
Yoshua
Bengio
processing
systems
2672–2680
2014
J
Jonathon
Shlens
Christian
Szegedy
Explaining
harnessing
adver-
sarial
arXiv:1412.6572
First
googling
Diederik
P
Max
Welling
Auto-encoding
variational
bayes
Yann
LeCun
Corinna
Cortes
Christopher
JC
Burges
mnist
handwritten
arXiv:1312.6114
2013.
digits
1998
neuron
Alec
Luke
Metz
Soumith
Chintala
Unsupervised
convolutional
arXiv:1511.06434
2015
Wojciech
Zaremba
Joan
Bruna
Dumitru
Erhan
Intriguing
properties
Rob
Fergus
arXiv:1312.6199
2013
Tim
Urban
ai
revolution
road
superintelligence
Appendix
Because
wasn
Generated
pass
unlearn
passes
There
appears
evidence
visual
under-ﬁtting
repeated
textures
across
multiple
