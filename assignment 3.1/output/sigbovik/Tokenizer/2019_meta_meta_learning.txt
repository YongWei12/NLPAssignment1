Meta-meta-learning
for
Neural
Architecture
Search
through
arXiv
Descent
Antreas
Antoniou
MetaMind
aa
@
mm.ai
Nick
Pawlowski
Googel
x2
nick
x.x
James
Owers
Facebrook
AI
Research
Team
jim
fart.org
Jack
Turner
slow.ai
jack
Joseph
Mellor
Institute
of
Yellow
Jumpers
joe
anditwasall.yellow
Elliot
J.
Crowley
ClosedAI
elliot
closed.ai
Abstract
Recent
work
in
meta-learning
has
set
the
deep
learning
community
alight
.
From
minute
gains
on
few-shot
tasks
,
to
discovering
architectures
that
are
slightly
better
than
chance
solving
intelligence
itself1
is
proving
a
popular
solution
every
conceivable
problem
ever
conceivably
conceived
In
this
paper
we
venture
deeper
into
computational
insanity
metalearning
and
potentially
risk
exiting
simulation
reality
itself
by
attempting
meta-learn
at
third
level
We
showcase
resulting
approach—which
call
meta-meta-learning—for
neural
architecture
search
Crucially
instead
differentiably
as
DARTS
(
Liu
et
al.
2018
)
meta-meta-learn
an
searching
This
descent
GPU-free
only
requires
handful
graduate
students
Further
introduce
regulariser
called
college-dropout
which
works
randomly
removing
single
student
from
our
system
As
consequence
procrastination
levels
decrease
signiﬁcantly
due
increased
workload
sense
responsibility
each
attains
The
code
experiments
publicly
available
Edit
:
have
decided
not
release
concerned
it
may
be
used
malicious
purposes
1
Introduction
Meta-learning
originally
described
Donald
B.
Maudsley
1979
was
invented
Jürgen
Schmidhuber
1997
great
renaissance
idea
believed
come
him
residual
He
2016
effect
inhalation
cosmic
matter
originating
rift
space-time
caused
old
one
Shub-Niggurath
Lovecraft
&
Niggurath
1923
although
details
this—and
horrors
more
generally—are
beyond
scope
human
comprehension
Probably
DeepMind
wouldn
’
t
tell
us
when
asked
Preprint
Rejected
Figure
A
mammal
mistaken
MAML
algorithm
but
equally
difﬁcult
train
or
learn
post-GAN-hypetrain
paradigm
involving
approximately
two
abstraction
Consider
Finn
2017
objective
good
initial
weights
network
such
can
quickly
adapt
classiﬁcation
task
unseen
data
lower
case
individual
training
higher
Hintonian
across-task
information
involves
calculating
some
second-order
derivatives
fortunately
autograd
means
don
understand
what
actually
going
An
illustration
given
clarity
—not
darts
Wikipedia
2019
—
performs
NAS
Zoph
;
Wu
Zhang
similar
manner
with
classifying
32⇥32
images
frogs
boats
Krizhevsky
2009
—a
naturally
extends
whole
host
real-world
applications—and
do
explicitly
add
another
sycophantically
term
Schmidhubrian
At
high
arXiv—a
process
Descent—for
papers
learn-to-learn
networks
perform
optimally
always
CIFAR
Omniglot
variant
ImageNet
narrows
down
somewhat
Once
they
obtained
pass
usually
collaborating
being
supervised
Schmidhubrian-level
will
apply
selected
learning-to-learn
novel
new
tasks/CIFAR-10
If
sampled
then
just
use
CapsulesNet
Sabour
fun
Finally
lowest
trained
using
carefully
thought-out2
hyperparameters
2
Method
begin
writing
project
proposal
MSc
PhD
submitted
interview
procedures
stage
multitude
PhD/MSc
examined
their
ability
digest
highly
complex
literature
produce
creative
solutions
previously
problems3
consistently
reliably
average
90
hours
week
18
day4
interviews
completed
mostly
chose
liked
most
based
anything
other
quantitative/objective
teach
how
descend
follows
ﬁrst
identiﬁer
initialised
following
Xavier
uniform
scheme
digits
year
YY
month
MM
period
4
digit
submission
number
harm
climate
running
extensive
optimisation
amount
GPUs
major
requirement
meta-meta-learning
industry
standard
ﬁeld
see
https
//twitter.com/twinaki/status/908085572283092996
Ability
distinguish
32x32
pixel
All
techniques
Grad
Geo�
Hinton
Pareto-optimal
grad
frog
classification
ratio
culled
Experimental
results
had
datapoints
so
took
liberty
ﬁtting
green
curve
them
star
shows
all
because
re
same
random
Graduate
iterate
accessing
vaguely
related
image
computer
vision
setup
probability
p
adaptarchitecture
|
CVpaper
=
⇡adapt
alternatively
4-digit
Decreasing
leads
earlier
Earlier
often
ﬂag-planting
methodology
half-baked
desirable
increases
rules
Gregorian
calendar
hope
ﬁnding
pretty
pictures
By
increasing
date
increase
hitting
published
within
GAN-hype
led
generation
many
without
any
real
application
5
Nevertheless
therefore
hold
useful
implement
early
stopping
Caruana
2001
ﬁnely
cherry-picking
best
suit
hypothesis
cases
where
converging
fast
enough
also
several
arbitrary
both
bewilder
reduce
internal
covariate
shift
dropped
out
become
unable
afford
completely
insane
fees
programme
Results
found
AmoebaNet
Real
quite
Our
observed
Rethinking
Meta-Meta-Learning
Meta-meta
recently
been
proposed
Because
research
saturated
few
months
someone
write
disputing
method
fashionable
easier
thinking
up
something
original
far
authors
DeepFakes
constitute
Related
Work
entirely
why
“
”
section
placed
end
afterthought
previous
acknowledge
act
unnecessary
self-citation
barely
relevant
2015
Conclusion
It
should
obvious
now
decreasing
size
sections
indicate
steam
shall
conclude
technique
really
future
consist
whatever
think
next
References
R.
Lawrence
S.
Giles
C.
L.
Overﬁtting
nets
Backpropagation
conjugate
gradient
Advances
Information
Processing
systems
E.
N.
ensembles
behave
like
colony
bees
Retreats
Systems
Abbeel
P.
Levine
Model-agnostic
adaptation
International
Conference
Machine
Learning
K.
X.
Ren
Sun
J
Deep
recognition
Proceedings
IEEE
Computer
Vision
Pattern
Recognition
multiple
layers
features
tiny
Master
s
thesis
University
Toronto
H.
Simonyan
Yang
Y.
Differentiable
preprint
arXiv:1806.09055
colour
arXiv:2311.01234
Aggarwal
A.
Huang
Le
Q.
V.
Regularized
evolution
classiﬁer
arXiv:1802.01548
Frosst
G.
Dynamic
routing
between
capsules
processing
Musings
Darts
//en.wikipedia.org/wiki/Darts
Dai
Wang
F.
Tian
Vajda
Jia
Keutzer
FBNet
Hardware-aware
efﬁcient
convnet
design
via
differentiable
arXiv:1812.03443
Z.
You
once
Single
shot
direct
sparse
optimization
arXiv:1811.01567
Vasudevan
Shlens
transferable
scalable
