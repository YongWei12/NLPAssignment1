Proceedings
of
SIGBOVIK
2019
A
LL
YOU
N
EED
IS
D
OGBALL
Kai
Arulkumaran
Imperial
College
London
,
UK
ka709
@
ic.ac.uk
Matthew
Kelcey
Victoria
Australia
matthew.kelcey
gmail.com
Andrew
Brock
Heriot-Watt
University
Edinburgh
ajb5
hw.ac.uk
BSTRACT
The
year
is
and
humanity
on
the
brink
destruction
.
latter
fact
has
nothing
to
do
with
current
state
AI
but
if
we
manage
survive
next
few
decades
it
may
well
If
can
now
beat
us
at
our
own
simulated
war
games
or
convince
that
there
exists
a
secret
herd
unicorns
in
South
America
what
?
All
things
considered
propose
reasonable
option
always
look
bright
side
life
More
speciﬁcally
chronicle
here
conception
well-loved
creation
Dogball
its
later
adventures
interwebs
I
NTRODUCTION
First
were
told
attention
all
you
need
(
Vaswani
et
al.
2017
)
Then
just
maybe
didn
’
t
Press
&
Smith
2018
And
somewhere
along
way
also
needed
was
CNNs
Chen
Wu
by
point
Bored
Yann
LeCun
getting
little
repetitive
kind
ignored
one
#
torched
So
re
say
because
YOLO
Redmon
Farhadi
origins
lie
Inception
wars
2016-2018
Salimans
2016
which
research
groups
worldwide
competing
make
prettiest
most
high
resolution
faces
name
science
Despite
heroic
efforts
reduce
GAN
violence
Albanie
arms
race
escalated
recent
years
culminating
notorious
BigGAN1
With
fell
swoop
lot
TPUs
Buchlovsky
BigGAN
blew
other
GANs
out
water
putting
an
end
conﬂict2
Many
experiments
went
into
ﬁnal
models
these
duly
chronicled
appendix
Indeed
community
noted
level
detail
available
feat
usually
reserved
for
works
Hochreiter
Klambauer
Experiments
ranged
over
hyperparameters
regularisation
strategies
noise
distributions3
much
more
However
serendipitous
ﬁnding
from
middle
training
his
family
chimeras
see
Figure
1
result
phenomenon
named
class
leakage
bringing
literal
meaning
maxim
deep
learning
alchemy
b
Catﬂower
c
Hendog
d
Nope
:
portraits
a-c
are
members
classus
leakus
extended
father
s
Whose
aliases
include
“
BFG
”
big
feedforward
At
least
until
StyleGAN
showed
up
months
Karras
3
RIP
Bernie
Bernoulli
your
hypercubic
binary
latent
space
beautiful
alas
not
amenable
truncation
trick
C
ULTURAL
MPACT
Deep
no
stranger
whimsy
From
ﬁguratively
Kaiser
;
Schmidhuber
literally
outrageous
Shazeer
names
researchers
fond
their
wordplay
Donahue
Tomczak
Welling
fan
animals
veritable
zoo
including
MAMLs
Finn
Reptiles
Nichol
Schulman
SNAILs
Mishra
even
DRAGANs
Kodali
Given
this
perhaps
inevitable
could
put
seriousness
aside
moment4
relish
glory
2
only
useful
making
pretty
pict-ALL
GLORY
TO
THE
DOGBALL
!
appealed
through
classic
memes
pop
culture
references
4
small
backlash
proliferation
5
resistance
futile
6
eventually
assimilated
7
On
phone
English-speaking
American
another
example
AI-created
Radford
Current
topics
included
how
many
meta-
meta-learning
algorithm
choosing
Sesame
Street
character
new
NLP
model
after
DogBall
Captain
Fear
Las
Vegas
Films
parables
modern
times
source
common
adage
dodge
wrench
Dogball.
about
dressing
job
want
drugs
bad
OK
Oh
my
God
Mat
You
ask
people
stop
dogball
Likelihood-based
have
chance
time.
Honestly
neither
CONCLUSION
short-lived
nature
fame
internet
shorter-lived
state-of-the-art
results
legacy
lives
various
places
such
as
custom
emoji
lab
Slack
channels
authors
hope
work
serves
reminder
once
while
nice
instead
frivolous
uses
R
EFERENCES
Samuel
Sébastien
Ehrhardt
João
F
Henriques
Stopping
Violence
Generative
Unadversarial
Networks
In
Jeff
Karen
Simonyan
Large
Scale
Training
High
Fidelity
Natural
Image
Synthesis
ICLR
Peter
David
Budden
Dominik
Grewe
Chris
Jones
John
Aslanides
Frederic
Besse
Andy
Aidan
Clark
Sergio
Gómez
Colmenarejo
Aedan
Pope
al
TF-Replicator
Distributed
Machine
Learning
Researchers
arXiv
preprint
arXiv:1902.00465
Qiming
Ren
CNN
Is
Need
arXiv:1712.09662
Zachary
Lipton
Julian
McAuley
Dance
Convolution
ICML
pp
1039–1048
Chelsea
Pieter
Abbeel
Sergey
Levine
Model-agnostic
Meta-learning
Fast
Adaptation
Lukasz
Gomez
Noam
Ashish
Niki
Parmar
Llion
Jakob
Uszkoreit
One
Model
Learn
Them
arXiv:1706.05137
Tero
Samuli
Laine
Timo
Aila
Style-based
Generator
Architecture
Adversarial
arXiv:1812.04948
Günter
Thomas
Unterthiner
Andreas
Mayr
Sepp
Self-normalizing
Neural
NIPS
971–980
Naveen
Jacob
Abernethy
James
Hays
Zsolt
Kira
Convergence
Stability
arXiv:1705.07215
Nikhil
Mostafa
Rohaninejad
Xi
Simple
Attentive
Meta-learner
Alex
Reptile
Scalable
Metalearning
Algorithm
arXiv:1803.02999
Oﬁr
Noah
May
Not
Attention
arXiv:1810.13409
Jeffrey
Rewon
Child
Luan
Dario
Amodei
Ilya
Sutskever
Language
Models
Unsupervised
Multitask
Learners
Technical
report
OpenAI
Joseph
Ali
YOLOv3
An
Incremental
Improvement
arXiv:1804.02767
Tim
Ian
Goodfellow
Wojciech
Zaremba
Vicki
Cheung
Alec
Improved
Techniques
2234–2242
Juergen
Big
Net
For
Everything
arXiv:1802.08864
Azalia
Mirhoseini
Krzysztof
Maziarz
Davis
Quoc
Le
Geoffrey
Hinton
Dean
Outrageously
Sparsely-gated
Mixture-of-experts
Layer
Jakub
Max
VAE
VampPrior
AISTATS
1214–1223
Łukasz
Illia
Polosukhin
5998–6008
